{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 02 Part 2: Neural Net Template\n",
    "\n",
    "For this assignment I tried a few different approaches to building neural network digit classifiers. I built a binary classifier, working similarly to the perceptron but with hidden layers, a one-hot encoded softmax classifier, and a multi-class real-number classifier.\n",
    "\n",
    "The scores are compiled below the code along side reflections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data + Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Imports'''\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "'''Data'''\n",
    "#loading training data inputs + labels \n",
    "train_data = np.loadtxt(\"mnist_train.csv\", delimiter = \",\")\n",
    "train_input = [ d[1:] for d in train_data ]\n",
    "#stacking 1s on as biases\n",
    "train_input = (np.hstack((np.ones((60000, 1)),train_input)))\n",
    "\n",
    "#loading testing data inputs + labels \n",
    "test_data = np.loadtxt(\"mnist_test.csv\", delimiter = \",\")\n",
    "test_input = [ d[1:] for d in test_data ]\n",
    "#stacking 1s on as biases\n",
    "test_input = (np.hstack((np.ones((10000, 1)),test_input)))\n",
    "\n",
    "'''Binary'''\n",
    "target_digit = 7\n",
    "train_label_binary = np.array([ int(d[0] == target_digit) for d in train_data]).T\n",
    "test_label_binary = np.array([ int(d[0] == target_digit) for d in test_data]).T\n",
    "\n",
    "'''MultiClass'''\n",
    "train_label_multi = np.array([ int(d[0]) for d in train_data]).T\n",
    "test_label_multi = np.array([ int(d[0]) for d in test_data]).T\n",
    "\n",
    "'''One Hot'''\n",
    "train_label_one_hot = np.asfarray(train_data[:, :1])\n",
    "test_labels_one_hot = np.asfarray(test_data[:, :1])\n",
    "\n",
    "# transform labels into one hot representation\n",
    "lr = np.arange(10)\n",
    "train_labels_one_hot = (lr==train_label_one_hot).astype(np.float)\n",
    "test_labels_one_hot = (lr==test_labels_one_hot).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "class ANN:\n",
    "    \n",
    "    '''2.2. Complete the initialisation of the neural net.'''\n",
    "    \n",
    "    def __init__(self, no_inputs = 784, #plus bias...\n",
    "                 max_iterations = 65,\n",
    "                 config  = (70, 50, 50, 50, 28, 1), #no of layers, no of nodes in layer, one output \n",
    "                 learning_rate = 0.0003, #learning rate\n",
    "                 activation_function = \"relu\", \n",
    "                 target = 7): #or rectifier\n",
    "                \n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.config = config\n",
    "        self.activation_function = activation_function \n",
    "        self.target = target\n",
    "        self.no_inputs = no_inputs\n",
    "        \n",
    "        #initialising weights\n",
    "        #random numbers between 10 and -10 + stacked 1s for biases\n",
    "        self.w = []\n",
    "        for i in range(len(self.config)):\n",
    "            self.w.append(None)\n",
    "            \n",
    "        for i in range(len(self.config)):\n",
    "            if i == 0: \n",
    "                #self.w[i] = np.array(np.random.random((config[i], no_inputs)))\n",
    "                self.w[i] = (2*np.random.random((config[i], no_inputs)) - 1)/ no_inputs\n",
    "                self.w[i] = np.hstack((np.ones((config[i], 1)), self.w[i]))\n",
    "            else:\n",
    "                #self.w[i] = np.array(np.random.random(-1, 1, (config[i], config[i-1])))/config[i-1]\n",
    "                self.w[i] = (2*np.random.random((config[i], config[i-1])) - 1) / config[i-1]\n",
    "                self.w[i] = (np.hstack((np.ones((config[i], 1)), self.w[i])))                         \n",
    "\n",
    "        self.last_output = []\n",
    "        for i in range(len(self.w)):\n",
    "            self.last_output.append(None)\n",
    "            \n",
    "        self.error = []\n",
    "        for i in range(len(self.w)):\n",
    "            self.error.append(None)\n",
    "        \n",
    "        self.gv = []\n",
    "        for i in range(len(self.w)):\n",
    "            self.gv.append(None)\n",
    "    \n",
    "    ##============###\n",
    "    ##  Functions ###\n",
    "    ##============### \n",
    "    \n",
    "    def print_details(self):\n",
    "        print(\"Target:\\t\" + str(self.target))\n",
    "        print(\"No. inputs:\\t\" + str(self.no_inputs))\n",
    "        print(\"Max iterations:\\t\" + str(self.max_iterations))\n",
    "        print(\"Learning rate:\\t\" + str(self.learning_rate))\n",
    "        print(\"Architecture:\\t\" + str(self.config))\n",
    "\n",
    "    \n",
    "    def sigmoid(self, a):\n",
    "        return 1/ (1 + np.exp(-1 * a))\n",
    "    \n",
    "    '''2.5. Implement the rectifier activation function.\n",
    "    \n",
    "    The forward phase can be updated to use the alternate activation function via activation \n",
    "    parameter.\n",
    "    Backpropagation is also updated to use the alternative derivative via this activation \n",
    "    parameter.\n",
    "    > 95% accuracy is achieved with the binary and multiclass neural networks\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def rectifier(self, a):\n",
    "        return ((a > 0) * a)\n",
    "        \n",
    "    def dfrectifier(self, a):\n",
    "        return ((a > 0) * 1)\n",
    "    #===============================#\n",
    "    #feed forward\n",
    "    def prediction(self,x): \n",
    "        for i in range(len(self.w)):\n",
    "            #if first layer\n",
    "            if i == 0: \n",
    "                #a = weights * intputs\n",
    "                a = np.dot(x, self.w[i].T)\n",
    "            #for all other layers\n",
    "            else: \n",
    "                #a = weights * outputs of previous layers\n",
    "                a = np.dot(self.last_output[i-1], self.w[i].T)\n",
    "\n",
    "            #run through activation + store\n",
    "            if i < len(self.w)-1:\n",
    "                if self.activation_function == \"sigmoid\":\n",
    "                    self.last_output[i] = (np.hstack((np.ones((len(a),1)),self.sigmoid(a))))\n",
    "                else: #ReLu\n",
    "                    self.last_output[i] = (np.hstack((np.ones((len(a),1)),self.rectifier(a)))) \n",
    "            else:\n",
    "                self.last_output[i] = np.around(a) \n",
    "        return(self.last_output[len(self.w)-1])\n",
    "    #===============================#\n",
    "    \n",
    "    '''2.3. Complete the training implementation '''\n",
    "    \n",
    "    #sgd backpropogation\n",
    "    def train(self, train_inputs, train_labels):\n",
    "        #shuffle\n",
    "        train_inputs, train_labels = shuffle(train_inputs, train_labels) \n",
    "        #labels\n",
    "        labels = np.array([train_labels]).T\n",
    "        #for each iter\n",
    "        for iteration in range(self.max_iterations):\n",
    "            #for each input\n",
    "            for i in range(len(train_inputs)-1):\n",
    "                o = np.array([train_inputs[i]])\n",
    "                t = train_labels[i]\n",
    "                #for each layer\n",
    "                for i in range(len(self.w),0,-1):\n",
    "                    #if output layer (if i == 4)\n",
    "                    if i == len(self.w):\n",
    "                        #error = o-t\n",
    "                        self.error[i-1] = self.prediction(o) - t\n",
    "                        #gv = error * inputs\n",
    "                        self.gv[i-1] = np.dot(self.error[i-1].T, self.last_output[i-2])\n",
    "                    \n",
    "                    else: #if hidden layer \n",
    "                        error = self.error[i]#next error\n",
    "                        weights = self.w[i][:, 1:] #next weights\n",
    "                        #derivation of activation\n",
    "                        if self.activation_function == \"sigmoid\":\n",
    "                            df = self.last_output[i-1][:, 1:] * (1 - self.last_output[i-1][:, 1:])\n",
    "                        else: #rectifier\n",
    "                            df = self.dfrectifier(self.last_output[i-1][:, 1:])\n",
    "    \n",
    "                        error1 = error.dot(weights)\n",
    "                        self.error[i-1] = np.multiply(error1, df)\n",
    "                        \n",
    "                        #gv calculations\n",
    "                        if i == 1: \n",
    "                            self.gv[i-1] = np.dot(self.error[i-1].T, o)\n",
    "                        else:\n",
    "                            self.gv[i-1] = np.dot(self.error[i-1].T,  self.last_output[i-2])\n",
    "                    #weight updates\n",
    "                for i in range(len(self.gv)):\n",
    "                    self.w[i-1] = self.w[i-1] - (self.learning_rate * self.gv[i-1])\n",
    "                \n",
    "    #===============================#\n",
    "    \n",
    "    '''2.4. (3 marks) Complete the testing implementation.\n",
    "    \n",
    "    The forward phase is calculated for each item in the testing set by Prediction function\n",
    "    The accuracy, precision, and recall are printed\n",
    "    > 95% accuracy is achieved for the binary neural network\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #testing\n",
    "    def test(self, testing_data, labels):\n",
    "        assert len(testing_data) == len(labels)\n",
    "        o = self.prediction(testing_data)\n",
    "        example = []\n",
    "        for i in o.flatten()[:10]:\n",
    "            example.append(int(i))\n",
    "        print(\" \")\n",
    "        print(\"Predictions\", example)\n",
    "        t = np.array([labels]).T\n",
    "        examplet = []\n",
    "        for i in t.flatten()[:10]:\n",
    "            examplet.append(int(i))\n",
    "        print(\"True Labels\", examplet)\n",
    "\n",
    "        print(\"Accuracy:\\t\", metrics.accuracy_score(t, o))  \n",
    "        print(\"Precision:\\t\", metrics.precision_score(t, o, average = 'macro', \n",
    "                                                     zero_division = 1))\n",
    "        print(\"Recall:\\t\", metrics.recall_score(t, o, average = 'macro', \n",
    "                                                zero_division = 1))\n",
    "        print(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main method\n",
    "\n",
    "### 1. Binary Classifier\n",
    "\n",
    "This binary method is a direct comparison with the perceptron, which classified binary target digits, as 0,1, and iterated through the series of digits. A Relu NN performed much better than the perceptron, and a Sigmoid NN, which perhaps struggled with oscillating weights due to the increased neurons and layers.\n",
    "\n",
    "Sigmoid: 1 hidden layer of 35 nodes, a learning rate of 0.02, training for 17 iterations with the sigmoid function, scores ~ 96% accuracy.\n",
    "\n",
    "Relu: 3 hidden layers of 28 nodes, a learning rate of 0.001, and training for 5 iterations with the relu function, scores ~99% accuracy in identifying digits through binary classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create 10 NNs to identify 10 digits\"\"\"\n",
    "Nodes = []\n",
    "for i in range(10):\n",
    "    Nodes.append(ANN(activation_function = \"sigmoid\", config = (35, 1), \n",
    "        max_iterations = 17, learning_rate = 0.02, target = i))\n",
    "    \n",
    "\"\"\"Iterate through and train, test and print outputs\"\"\"\n",
    "for net in Nodes:\n",
    "    train_label = [ int(d[0] == net.target) for d in train_data ]\n",
    "    test_label = [ int(d[0] == net.target) for d in test_data ]\n",
    "\n",
    "    print(\"-----\")\n",
    "    net.print_details()\n",
    "    net.test(test_input, test_label)\n",
    "    net.train(train_input, train_label)\n",
    "    net.test(test_input, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create 10 NNs to identify 10 digits\"\"\"\n",
    "Nodes = []\n",
    "for i in range(10):\n",
    "    Nodes.append(ANN(activation_function = \"relu\", config = (28, 28, 28, 1), \n",
    "        max_iterations = 5, learning_rate = 0.001, target = i))\n",
    "    \n",
    "\"\"\"Iterate through and train, test and print outputs\"\"\"\n",
    "for net in Nodes:\n",
    "    train_label = [ int(d[0] == net.target) for d in train_data ]\n",
    "    test_label = [ int(d[0] == net.target) for d in test_data ]\n",
    "\n",
    "    print(\"-----\")\n",
    "    net.print_details()\n",
    "    net.test(test_input, test_label)\n",
    "    net.train(train_input, train_label)\n",
    "    net.test(test_input, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. One-Hot/SoftMax\n",
    "\n",
    "This method encodes the labels as a vector of numbers from 0.1 to 0.9 to represent the respective probabilities of each digit, similar to one-hot encoding methods. That is, how likely this input is digit x for all digits 0-9. This is also known as a softmax classification approach. \n",
    "\n",
    "A softmax method is outlined fully in *Python Machine Learning Tutorial*'s neural network chapter. I did not follow this tutorial in full, as I only came to the softmax approach late on in the assignment, and so most of my code deviates from theirs. For that reason, I have very different scores. \n",
    "\n",
    "Sigmoid: 140 neurons, a learning rate of 0.0005 and 10 iterations reached 92% accuracy. \n",
    "\n",
    "Relu: 4 hidden layers, with 50, 28, 28 and 28 nodes, a learning rate of 0.01 and 20 epochs reaches 95% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ANN(activation_function = \"sigmoid\", config = (140, 10), learning_rate = 0.0005, \n",
    "         max_iterations = 10)\n",
    "\n",
    "#calling testing + training methods\n",
    "print(\"-----\")\n",
    "net.test(test_input, test_labels_one_hot)\n",
    "net.train(train_input, train_labels_one_hot)\n",
    "net.test(test_input, test_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ANN(activation_function = \"relu\", config = (50, 28, 28, 28, 10), learning_rate = 0.01, \n",
    "         max_iterations = 20)\n",
    "\n",
    "#calling testing + training methods\n",
    "print(\"-----\")\n",
    "net.test(test_imgs, test_labels_one_hot)\n",
    "net.train(train_imgs, train_labels_one_hot)\n",
    "net.test(test_imgs, test_labels_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-Class\n",
    "\n",
    "This is a more complex approach, which classifies every digit as a real number, rather than a probability. This was a harder challenge, and took a long time to train. Sigmoid activation struggled with this, perhaps due to exploding gradients and oscillating weights due to the complexity of the problem and layers needed and could not reach a respectible score. Relu did manage it, however.\n",
    "\n",
    "A complex configuration of 5 hidden layers sizes 70, 50, 50 50,28, a learning rate of 0.0003 and 65 iterations with relu activation scores 96% accuracy in identifying digits through multiclass classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = ANN(activation_function = \"relu\", no_inputs = 784, \n",
    "                 max_iterations = 65,\n",
    "                 config  = (70, 50, 50, 50, 28, 1),\n",
    "                 learning_rate = 0.0003)\n",
    "\n",
    "#calling testing + training methods\n",
    "print(\"-----\")\n",
    "net.test(test_input, test_label_multi)\n",
    "net.train(train_input, train_label_multi)\n",
    "net.test(test_input, test_label_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ANN(activation_function = \"sigmoid\", no_inputs = 784, \n",
    "                 max_iterations = 80,\n",
    "                 config  = (28, 28, 1),\n",
    "                 learning_rate = 0.03)\n",
    "\n",
    "#calling testing + training methods\n",
    "print(\"-----\")\n",
    "net.test(test_input, test_label_multi)\n",
    "net.train(train_input, train_label_multi)\n",
    "net.test(test_input, test_label_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = {\n",
    "    'Perceptron Binary Classifier': ['Sigmoid: 98%', 'Step: 98%'],\n",
    "    'NN Binary Classifier':  ['Sigmoid: 96%', 'Relu: 99%'],\n",
    "    'NN OneHot': ['Sigmoid: 92.15%', 'Relu: 95.5%'],\n",
    "    'NN MultiClass Classifier': ['Sigmoid: --', 'Relu: 96%'],\n",
    "        \n",
    "        }\n",
    "df = pd.DataFrame(Results, columns = ['Perceptron Binary Classifier',\n",
    "                                      'NN Binary Classifier',\n",
    "                                      'NN OneHot',\n",
    "                                      'NN MultiClass Classifier', \n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Perceptron Binary Classifier</th>\n",
       "      <th>NN Binary Classifier</th>\n",
       "      <th>NN OneHot</th>\n",
       "      <th>NN MultiClass Classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sigmoid: 98%</td>\n",
       "      <td>Sigmoid: 96%</td>\n",
       "      <td>Sigmoid:</td>\n",
       "      <td>Sigmoid: --</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Step: 98%</td>\n",
       "      <td>Relu: 99%</td>\n",
       "      <td>Relu: 96</td>\n",
       "      <td>Relu: 96%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Perceptron Binary Classifier NN Binary Classifier  NN OneHot  \\\n",
       "0                 Sigmoid: 98%         Sigmoid: 96%  Sigmoid:    \n",
       "1                    Step: 98%            Relu: 99%   Relu: 96   \n",
       "\n",
       "  NN MultiClass Classifier  \n",
       "0              Sigmoid: --  \n",
       "1                Relu: 96%  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "#### 2.4. Complete the testing implementation.\n",
    "\n",
    "How much better are the results for digit recognition, compared to the single-layer perceptron?\n",
    "\n",
    "* For binary digit recognition, the optimal NN was more accurate, achieving ~99% accuracy, whereas the perceptron achieved just under 98% accuracy. \n",
    "* However, the main advantage of the NN over the perceptron is the option for multiple outputs, complex configurations and mulitiple classifications. With this, though it take much longer to train than the single-layer perceptron, the single NN is able to classify every digit from 0 to 9 with 96% accuracy. \n",
    "\n",
    "How did you modify the initial weights, learning rate, and iterations to achieve this?\n",
    "\n",
    "* The optimisation of the final binary model did not take too long, since the model learned fairly quickly with relu. Parameterising the configuration into a single vector made the process of finding the optimal architecture much easier than my original code (for which I had to manually change and update the architecutre throughout the code). \n",
    "* I started with the configuration, finding an optimum number of layers, and nodes. Then I gradually decreased the learning rate and increased the epochs. This was to ensure the gradient descent of error was measured and accurate, rather than wild and inconsistent.\n",
    "\n",
    "How much faster/slower is the training time, compared to the single-layer perceptron?\n",
    "\n",
    "* The single-layer perceptron was slower to learn than the sigmoid activation function for the binary neural network. Though there are far less parameters to tweak in the perceptron, the use of matrix multiplication sped up the process and learning significantly compared to the perceptron which did not make use of matrix multiplication. \n",
    "* However, the single-layer perceptron was far quicker than the multi-labeled classification. However, with the optimal architecture for multi-label classification below, there are approximately 65,000 parameters to tune, per 60,000 input at 65 iterations. So, no wonder it takes along time!\n",
    "\n",
    "How much quicker/slower does the learning converge, compared to the single-layer perceptron?\n",
    "\n",
    "* The binary classifier NN with sigmoid took about 1/3 of the iterations needed (15) for online-learning, as the single layer perceptron did for batch-learning (40). So the binary neural network was quicker to learn and converge. \n",
    "\n",
    "#### 2.5 Implement the rectifier activation function\n",
    "\n",
    "How much better are the results for digit recognition, compared to the sigmoid activation function?\n",
    "\n",
    "* The results are much better and much quicker. For instance, with the configuration and parameters set below, sigmoid would achieve 90% accuracy while the relu would reach 99% accuracy. Since Sigmoid took longer to train, it needed more iterations and a simpler structure to converge.\n",
    "* The best score achieved by relu was over 99% while the sigmoid struggled to reach over  97%.\n",
    "\n",
    "How much quicker/slower does the learning converge, compared to the sigmoid activation function?\n",
    "\n",
    "* The binary classifier NN with relu took about 1/4 of the iterations (5) the sigmoid needed (17). So the relu was much quicker to learn and converge. \n",
    "\n",
    "How did you modify the initial weights, learning rate, and iterations to achieve this?\n",
    "\n",
    "* As above, I started with the configuration, and tuned the learning rate and number of iterations to increase accuracy towards the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Python Machine Learning Tutorial (n.d) *Neural Network*. Available at https://www.python-course.eu/neural_network_mnist.php [Accessed 30/03/2021]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
